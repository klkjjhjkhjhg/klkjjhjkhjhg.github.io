<!DOCTYPE html>
<html>

<head>
    <title>Lianghao Zhang's Homepage</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/bootstrap-icons.css">

    <style>
        html,
        body {
            font-family: Arial, sans-serif;
            background-color: #f2f2f2;
        }
        
        #background {
            height: 100%;
            background: #fcfcfc;
            padding: 0;
        }
        
        h1 {
            font-size: 32px;
            font-weight: bold;
            margin-bottom: 10px;
        }
        
        h2 {
            font-size: 24px;
            margin-bottom: 10px;
        }
        
        p {
            font-size: 16px;
            line-height: 1.5;
            margin-bottom: 20px;
        }
        
        .highlight {
            color: #007bff;
            font-weight: bold;
        }
        
        .textsection {
            font-size: 14pt;
            /*font-weight: bold;*/
            color: #333;
            text-align: left;
            padding: 10px;
            background-color: #f5f5f5;
            border-radius: 5px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }
        
        .papericon {
            border-radius: 8px;
            -moz-box-shadow: 3px 3px 6px #888;
            -webkit-box-shadow: 3px 3px 6px #888;
            box-shadow: 3px 3px 6px #888;
            height: 150px;
            width: 200px;
        }
        
        .papercontent {
            margin-bottom: 30px;
        }
        
        .blockcontainer {
            width: 100%;
        }
    </style>
</head>

<body>
    <div id="background" class="container shadow-lg">
        <nav class="navbar navbar-expand-lg mb-4 shadow-sm navbar-dark navbar-text-light" style="padding: 0.2rem 0rem; background:#333A56; color: #fff">
            <div class="container">
                <a class="navbar-brand" href="#">Lianghao Zhang's Homepage</a>
                <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>
                <div class="collapse navbar-collapse" id="navbarNav">
                    <ul class="navbar-nav ml-auto">
                        <li class="nav-item">
                            <a class="nav-link" href="#about">About</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="#publications">Publications</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="#contact">Contact</a>
                        </li>
                    </ul>
                </div>
            </div>
        </nav>
        <div id="container" class="container row justify-content-center mx-auto ">
            <div id="metainfo" class="blockcontainer">
                <div class="container" style="padding: 0rem 1rem 2rem 2rem">
                    <div class="row">
                        <div class="col-md-1">

                        </div>
                        <div class="col-md-3">
                            <img class="image profile-img" src="imgs/my.png" alt="Profile Photo" style="width: 80%;">
                        </div>
                        <div class="col-md-3">

                        </div>
                        <div class="col-md-4">
                            <div class="text-left">
                                <h1>Lianghao Zhang</h1>
                                <p>Tianjin University <br> Researcher <br>Computer Graphics &amp; Computer Vision</p>
                            </div>
                            <div class="text-left" style="margin-top: 0px;">
                                <a href="mailto:lianghaozhang@tju.edu.cn" class="btn btn-light btn-circle btn-lg shadow-sm" style="margin-right: 30px;"><i class="bi bi-envelope"></i></a>
                                <a href="https://github.com/klkjjhjkhjhg" class="btn btn-light btn-circle btn-lg shadow-sm" style="margin-right: 30px;"><i class="bi bi-github"></i></a>
                            </div>
                            <!-- <p>If you have any interesting projects or opportunities, feel free to **get in touch** with me.</p> -->
                        </div>
                    </div>
                </div>
            </div>


            <div id="about" class="blockcontainer">
                <p class="textsectionheader2 textsection" style="width: 100%; font-weight: bold;"> About</p>
                <div class="container" style="padding: 0">
                    <p>Hello, I'm Lianghao Zhang, currently working at Xiaomi. I received my Ph.D. degree from Tianjin University, where I was advised by <a href="http://cic.tju.edu.cn/faculty/zhangjiawan/index.htm">Prof. Jiawan Zhang</a>. My research interests
                        lie in computer graphics and computer vision, focusing on material acquisition, appearance modeling, and 3D reconstruction. My long-term vision is to democratize material capture technologies, enabling everyone to participate in
                        3D content creation and enriching the digital content ecosystem with greater vitality and diversity.</p>
                </div>
            </div>

            <div id="publications" class="blockcontainer">
                <p class="textsectionheader2 textsection" style="width: 100%; font-weight: bold;"> Publications</p>
                <div class="papercontent container" style="padding: 0">
                    <div class="row">
                        <div class="col-md-3 text-center">
                            <img class="image papericon" src="imgs/zhanglianghao2025.png">
                        </div>
                        <div class="col-md-9">
                            <p style="font-size: 18px; font-weight: bold; margin-bottom: 5px">Sparse SVBRDF Acquisition via Importance-Aware Illumination Multiplexing</p>
                            <hr style="margin-top:0px; margin-bottom:5px">
                            <div class="text-left">
                                <div>
                                    <p>
                                        <a href="#" style="color:#000; font-weight: bold;">Lianghao Zhang</a>, Zixuan Wang, Fangzhou Gao,
                                        <a href="https://cgliwang.github.io">Li Wang</a>, Ruya Sun,
                                        <a href="http://cic.tju.edu.cn/faculty/zhangjiawan/index.htm">Jiawan Zhang</a>
                                        <br> To appear in ACM Trans. on Graphics (Proc. SIGGRAPH Asia 2025).
                                    </p>
                                </div>
                                <div id="zhanglianghao2025" class="collapse" , style="text-align: justify;">
                                    <p>
                                        Reflectance acquisition from sparse images has been a long-standing problem in computer graphics. Previous works have addressed this by introducing either material-related priors or illumination multiplexing with a general sampling strategy. However,
                                        fixed lighting patterns in multiplexing can lead to redundant sampling and entangled observations, making it necessary to adaptively capture salient reflectance responses in each shot based on material behavior.
                                        In this paper, we propose combining adaptive sampling with illumination multiplexing for SVBRDF reconstruction from sparse images lit by a planar light source. Central to our method is the modeling of a sampling
                                        importance distribution on lighting surface, guided by the statistical nature of microfacet theory. Based on this sampling structure, our framework jointly trains networks to learn an adaptive sampling strategy
                                        in the lighting domain, and furthermore, approximately separates pure specular-related information from observations to reduce ambiguities in reconstruction. We validate our approach through experiments and comparisons
                                        with previous works on both synthetic and real materials.
                                    </p>
                                </div>
                                <div class="buttongroup">
                                    <button class="btn btn-dark" data-toggle="collapse" data-target="#zhanglianghao2025">Abstract</button>
                                    <a href="http://cic.tju.edu.cn/faculty/zhangjiawan/Jiawan_Zhang_files/paper/zhanglianghao2025.pdf" class="btn btn-primary">Paper</a>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="papercontent container" style="padding: 0">
                    <div class="row">
                        <div class="col-md-3 text-center">
                            <img class="image papericon" src="imgs/wangli2025.png">
                        </div>
                        <div class="col-md-9">
                            <p style="font-size: 18px; font-weight: bold; margin-bottom: 5px">EBREnv: SVBRDF Estimation in Uncontrolled Environment Lighting via Exemplar-Based Representation</p>
                            <hr style="margin-top:0px; margin-bottom:5px">
                            <div class="text-left">
                                <div>
                                    <p>
                                        <a href="https://cgliwang.github.io">Li Wang</a>, Jiajun Zhao
                                        <a href="#" style="color:#000; font-weight: bold;">Lianghao Zhang</a>, Fangzhou Gao,
                                        <a href="http://cic.tju.edu.cn/faculty/zhangjiawan/index.htm">Jiawan Zhang</a>
                                        <br> To appear in ACM SIGGRAPH Asia 2025 Conference Proceedings.
                                    </p>
                                </div>
                                <div id="wangli2025" class="collapse" , style="text-align: justify;">
                                    <p>
                                        Recovering spatial-varying bi-directional reflectance distribution function (SVBRDF) from as few as possible captured images has been a challenging task in computer graphics. Benefiting from the co-located flashlight-camera capture strategy and data-driven
                                        priors, SVBRDF can be estimated from few input images. However, this capture strategy usually requires a controllable darkroom environment, ensuring the flashlight is a single light source. It is often impractical
                                        during on-site capture in real-world scenarios. To support SVBRDF estimation in an uncontrolled environment, the key challenge lies in the high-precise estimation of unknown environment lighting and its effective
                                        utilization on SVBRDF recovery. To address this issue, we proposed a novel exemplar-based environment lighting representation, which is easier to use for neural networks. These exemplars are a set of rendered images
                                        of selected materials under the environment lighting. By embedding the rendering process, our approach transforms environment lighting represented in the spherical domain into the sample-surface domain, thereby
                                        achieving the domain alignment with input images. This significantly reduces the net- work’s learning burden, resulting in a more precise environment lighting estimation. Furthermore, after lighting prediction,
                                        we also present a dominant lighting extraction algorithm and an adaptive exemplar selection algorithm to enhance the guidance of environment lighting in SVBRDF estimation. Finally, considering the distant contribution
                                        of environment lighting and point lighting to SVBRDF recovery, we proposed a well-designed cascaded network. Quantitative assessments and qualitative analysis have demonstrated that our method achieves superior
                                        SVBRDF estimations compared to previous approaches. The source code will be released.
                                    </p>
                                </div>
                                <div class="buttongroup">
                                    <button class="btn btn-dark" data-toggle="collapse" data-target="#wangli2025">Abstract</button>
                                    <a href="http://cic.tju.edu.cn/faculty/zhangjiawan/Jiawan_Zhang_files/paper/wangli2025.pdf" class="btn btn-primary">Paper</a>
                                    <!-- <a href="#" class="btn btn-success">Code (Coming soon) </a> -->
                                </div>
                                <!-- <a href="#" class="btn btn-secondary"></a> -->
                            </div>
                        </div>
                    </div>
                </div>
                <div class="papercontent container" style="padding: 0">
                    <div class="row">
                        <div class="col-md-3 text-center">
                            <img class="image papericon" src="imgs/gaofangzhou2025.png">
                        </div>
                        <div class="col-md-9">
                            <p style="font-size: 18px; font-weight: bold; margin-bottom: 5px">RCTrans: Transparent Object Reconstruction in Natural Scene via Refractive Correspondence Estimation</p>
                            <hr style="margin-top:0px; margin-bottom:5px">
                            <div class="text-left">
                                <div>
                                    <p>
                                        Fangzhou Gao, Yuzhen Kang, <a href="#" style="color:#000; font-weight: bold;">Lianghao Zhang</a>, <a href="https://cgliwang.github.io ">Li Wang</a>, Qishen Wang,
                                        <a href="http://cic.tju.edu.cn/faculty/zhangjiawan/index.htm ">Jiawan Zhang</a>
                                        <br> To appear in ACM SIGGRAPH Asia 2025 Conference Proceedings.
                                    </p>
                                </div>
                                <div id="gaofangzhou2025" class="collapse" , style="text-align: justify;">
                                    <p>
                                        Transparent object reconstruction in an uncontrolled natural scene is a challenging task due to its complex appearance. Existing methods optimize the object shape with RGB color as supervision, which suffer from locality and ambiguity, and fail to recover
                                        accurate structures. In this paper, we present RCTrans, which uses ray-background intersection as a more efficient constraint to achieve high-quality reconstruction, while maintaining a convenient setup. The key
                                        technology to achieve this is a novel pre-trained correspondence estimation network, which allows us to acquire ray-background correspondence under uncontrolled scenes and camera views. In addition, a confidence
                                        evaluation is introduced to protect the reconstruction from inaccurate estimated correspondence. Extensive experiments on both synthetic and real data demonstrate that our method can produce highly accurate results,
                                        without any extra acquisition burden. The code and dataset will be publicly available.
                                    </p>
                                </div>
                                <div class="buttongroup">
                                    <button class="btn btn-dark" data-toggle="collapse" data-target="#gaofangzhou2025">Abstract</button>
                                    <a href="http://cic.tju.edu.cn/faculty/zhangjiawan/Jiawan_Zhang_files/paper/gaofangzhou2025.pdf" class="btn btn-primary">Paper</a>
                                    <!-- <a href="#" class="btn btn-success">Code (Coming soon) </a> -->
                                </div>
                                <!-- <a href="#" class="btn btn-secondary"></a> -->
                            </div>
                        </div>
                    </div>
                </div>
                <div class="papercontent container" style="padding: 0">
                    <div class="row">
                        <div class="col-md-3 text-center">
                            <img class="image papericon" src="imgs/zhanglianghao2025-2.png">
                        </div>
                        <div class="col-md-9">
                            <p style="font-size: 18px; font-weight: bold; margin-bottom: 5px">On-site single image SVBRDF reconstruction with active planar lighting</p>
                            <hr style="margin-top:0px; margin-bottom:5px">
                            <div class="text-left">
                                <div>
                                    <p>
                                        <a href="#" style="color:#000; font-weight: bold;">Lianghao Zhang</a>, Ruya Sun, <a href="https://cgliwang.github.io ">Li Wang</a>, Fangzhou Gao, Zixuan Wang,
                                        <a href="http://cic.tju.edu.cn/faculty/zhangjiawan/index.htm ">Jiawan Zhang</a>
                                        <br> Computers & Graphics 130 (2025) 104268.
                                    </p>
                                </div>
                                <div id="zhanglianghao2025-2" class="collapse" , style="text-align: justify;">
                                    <p>
                                        Recovering the spatially-varying bidirectional reflectance distribution function (SVBRDF) from a single image in uncontrolled environments is challenging while essential for various applications. In this paper, we address this highly ill-posed problem
                                        using a convenient capture setup and a carefully designed reconstruction framework. Our proposed setup, which incorporates an active extended light source and a mirror hemisphere, is easy to implement for even common
                                        users and requires no careful calibration. These devices can simultaneously capture uncontrolled lighting, real active lighting patterns, and material appearance in a single image. Based on all captured information,
                                        we solve the reconstruction problem by designing lighting clues that are semantically aligned with the input image to aid the network in understanding the captured lighting. We further embed lighting clue generation
                                        into the network’s forward pass by introducing real-time rendering. This allows the network to render accurate lighting clues based on predicted normal variations while jointly learning to reconstruct high-quality
                                        SVBRDF. Moreover, we also use captured lighting patterns to model noises of pattern display in real scenes, which significantly increases the robustness of our methods on real data. With these innovations, our method
                                        demonstrates clear improvements over previous approaches on both synthetic and real-world data.
                                    </p>
                                </div>
                                <div class="buttongroup">
                                    <button class="btn btn-dark" data-toggle="collapse" data-target="#zhanglianghao2025-2">Abstract</button>
                                    <a href="https://www.sciencedirect.com/science/article/abs/pii/S0097849325001098" class="btn btn-primary">Paper</a>
                                    <!-- <a href="#" class="btn btn-success">Code (Coming soon) </a> -->
                                </div>
                                <!-- <a href="#" class="btn btn-secondary"></a> -->
                            </div>
                        </div>
                    </div>
                </div>
                <div class="papercontent container" style="padding: 0">
                    <div class="row">
                        <div class="col-md-3 text-center">
                            <img class="image papericon" src="imgs/guoziheng2025.png">
                        </div>
                        <div class="col-md-9">
                            <p style="font-size: 18px; font-weight: bold; margin-bottom: 5px">PixelatedScatter: Arbitrary-level Visual Abstraction for Large-scale Multiclass Scatterplots</p>
                            <hr style="margin-top:0px; margin-bottom:5px">
                            <div class="text-left">
                                <div>
                                    <p>
                                        Ziheng Guo, Tianxiang Wei, Zeyu Li, <a href="#" style="color:#000; font-weight: bold;">Lianghao Zhang</a>, Sisi Li,
                                        <a href="http://cic.tju.edu.cn/faculty/zhangjiawan/index.htm ">Jiawan Zhang</a>
                                        <br> To appear in IEEE Trans. Vis. Comput. Graph.
                                    </p>
                                </div>
                                <div id="guoziheng2025" class="collapse" , style="text-align: justify;">
                                    <p>
                                        Recovering the spatially-varying bidirectional reflectance distribution function (SVBRDF) from a single image in uncontrolled environments is challenging while essential for various applications. In this paper, we address this highly ill-posed problem
                                        using a convenient capture setup and a carefully designed reconstruction framework. Our proposed setup, which incorporates an active extended light source and a mirror hemisphere, is easy to implement for even common
                                        users and requires no careful calibration. These devices can simultaneously capture uncontrolled lighting, real active lighting patterns, and material appearance in a single image. Based on all captured information,
                                        we solve the reconstruction problem by designing lighting clues that are semantically aligned with the input image to aid the network in understanding the captured lighting. We further embed lighting clue generation
                                        into the network’s forward pass by introducing real-time rendering. This allows the network to render accurate lighting clues based on predicted normal variations while jointly learning to reconstruct high-quality
                                        SVBRDF. Moreover, we also use captured lighting patterns to model noises of pattern display in real scenes, which significantly increases the robustness of our methods on real data. With these innovations, our method
                                        demonstrates clear improvements over previous approaches on both synthetic and real-world data.
                                    </p>
                                </div>
                                <div class="buttongroup">
                                    <button class="btn btn-dark" data-toggle="collapse" data-target="#guoziheng2025">Abstract</button>
                                    <a href="http://cic.tju.edu.cn/faculty/zhangjiawan/Jiawan_Zhang_files/paper/guoziheng2025.pdf" class=" btn btn-primary ">Paper</a>
                                    <!-- <a href="# " class="btn btn-success ">Code (Coming soon) </a> -->
                                </div>
                                <!-- <a href="# " class="btn btn-secondary "></a> -->
                            </div>
                        </div>
                    </div>
                </div>
                <div class="papercontent container " style="padding: 0 ">
                    <div class="row ">
                        <div class="col-md-3 text-center ">
                            <img class="image papericon " src="imgs/wangli2024.png ">
                        </div>
                        <div class="col-md-9 ">
                            <p style="font-size: 18px; font-weight: bold; margin-bottom: 5px ">NFPLight: Deep SVBRDF Estimation via the Combination of Near and Far Field Point Lighting</p>
                            <hr style="margin-top:0px; margin-bottom:5px ">
                            <div class="text-left ">
                                <div>
                                    <p>
                                        <a href="https://cgliwang.github.io ">Li Wang</a>,
                                        <a href="# " style="color:#000; font-weight: bold; ">Lianghao Zhang</a>, Fangzhou Gao, Yuzhen Kang,
                                        <a href="http://cic.tju.edu.cn/faculty/zhangjiawan/index.htm ">Jiawan Zhang</a>
                                        <br> ACM Trans. on Graphics (Proc. SIGGRAPH Asia 2024), 43, 6.
                                    </p>
                                </div>
                                <div id="wangli2024 " class="collapse " , style="text-align: justify; ">
                                    <p>
                                        Recovering spatial-varying bi-directional reflectance distribution function (SVBRDF) from a few hand-held captured images has been a challenging task in computer graphics. Benefiting from the learned priors from data, single-image methods can obtain plausible
                                        SVBRDF estimation results. However, the extremely limited appearance information in a single image does not suffice for high-quality SVBRDF reconstruction. Although increasing the number of inputs can improve the
                                        reconstruction quality, it also affects the efficiency of real data capture and adds significant computational burdens. Therefore, the key challenge is to minimize the required number of inputs, while keeping high-quality
                                        results. To address this, we propose maximizing the effective information in each input through a novel co-located capture strategy that combines near-field and far-field point lighting. To further enhance effectiveness,
                                        we theoretically investigate the inherent relation between two images. The extracted relation is strongly correlated with the slope of specular reflectance, substantially enhancing the precision of roughness map
                                        estimation. Additionally, we designed the registration and denoising modules to meet the practical requirements of hand-held capture. Quantitative assessments and qualitative analysis have demonstrated that our
                                        method achieves superior SVBRDF estimations compared to previous approaches. All source codes will be publicly released.
                                    </p>
                                </div>
                                <div class="buttongroup ">
                                    <button class="btn btn-dark " data-toggle="collapse " data-target="#wangli2024 ">Abstract</button>
                                    <a href="https://cgliwang.github.io/NFPLight/ " class="btn btn-primary ">Project</a>
                                    <!-- <a href="# " class="btn btn-success ">Code (Coming soon) </a> -->
                                </div>
                                <!-- <a href="# " class="btn btn-secondary "></a> -->
                            </div>
                        </div>
                    </div>
                </div>
                <div class="papercontent container " style="padding: 0 ">
                    <div class="row ">
                        <div class="col-md-3 text-center ">
                            <img class="image papericon " src="imgs/chengjiamin2024.png ">
                        </div>
                        <div class="col-md-9 ">
                            <p style="font-size: 18px; font-weight: bold; margin-bottom: 5px ">Single-image SVBRDF estimation with auto-adaptive high-frequency feature extraction</p>
                            <hr style="margin-top:0px; margin-bottom:5px ">
                            <div class="text-left ">
                                <div>
                                    <p>
                                        Jiamin Cheng,
                                        <a href="https://cgliwang.github.io ">Li Wang</a>,
                                        <a href="# " style="color:#000; font-weight: bold; ">Lianghao Zhang</a>, Fangzhou Gao,
                                        <a href="http://cic.tju.edu.cn/faculty/zhangjiawan/index.htm ">Jiawan Zhang</a>
                                        <br> Computers & Graphics 124 (2024) 104103.
                                    </p>
                                </div>
                                <div id="chengjiamin2024 " class="collapse " , style="text-align: justify; ">
                                    <p>
                                        In this paper, we address the task of estimating spatially-varying bi-directional reflectance distribution functions (SVBRDF) of a near-planar surface from a single flash-lit image. Disentangling SVBRDF from the material appearance by deep learning has
                                        proven a formidable challenge. This difficulty is particularly pronounced when dealing with images lit by a point light source because the uneven distribution of irradiance in the scene interacts with the surface,
                                        leading to significant global luminance variations across the image. These variations may be overemphasized by the network and wrongly baked into the material property space. To tackle this issue, we propose a high-frequency
                                        path that contains an auto-adaptive subband ‘‘knob’’. This path aims to extract crucial image textures and details while eliminating global luminance variations present in the original image. Furthermore, recognizing
                                        that color information is ignored in this path, we design a two-path strategy to jointly estimate material reflectance from both the high-frequency path and the original image. Extensive experiments on a substantial
                                        dataset have confirmed the effectiveness of our method. Our method outperforms state-of-the-art methods across a wide range of materials.
                                    </p>
                                </div>
                                <div class="buttongroup ">
                                    <button class="btn btn-dark " data-toggle="collapse " data-target="#chengjiamin2024 ">Abstract</button>
                                    <a href="https://doi.org/10.1016/j.cag.2024.104103 " class="btn btn-primary ">Project</a>
                                    <!-- <a href="# " class="btn btn-success ">Code (Coming soon) </a> -->
                                </div>
                                <!-- <a href="# " class="btn btn-secondary "></a> -->
                            </div>
                        </div>
                    </div>
                </div>
                <div class="papercontent container " style="padding: 0 ">
                    <div class="row ">
                        <div class="col-md-3 text-center ">
                            <img class="image papericon " src="imgs/zhanglianghao2023.png ">
                        </div>
                        <div class="col-md-9 ">
                            <p style="font-size: 18px; font-weight: bold; margin-bottom: 5px ">Deep SVBRDF Estimation from Single Image under Learned Planar Lighting</p>
                            <hr style="margin-top:0px; margin-bottom:5px ">
                            <div class="text-left ">
                                <div>
                                    <p>
                                        <a href="# " style="color:#000; font-weight: bold; ">Lianghao Zhang</a>, Fangzhou Gao,
                                        <a href="https://cgliwang.github.io ">Li Wang</a>,
                                        <a href="http://cic.tju.edu.cn/faculty/yuminjing/index.html ">Minjing Yu</a>, Jiamin Cheng,
                                        <a href="http://cic.tju.edu.cn/faculty/zhangjiawan/index.htm ">Jiawan Zhang</a>
                                        <br> ACM SIGGRAPH 2023 Conference Proceedings, Article 48, 1-11.
                                    </p>
                                </div>
                                <div id="zhanglianghao2023 " class="collapse " , style="text-align: justify; ">
                                    <p>
                                        Estimating spatially varying BRDF from a single image without complicated acquisition devices is a challenging problem. In this paper, a deep learning based method was proposed to improve the capture efficiency of single image significantly by learning
                                        the lighting pattern of a planar light source, and reconstruct high-quality SVBRDF by learning the global correlation prior of the input image. In our framework, the lighting pattern optimization is embedded in
                                        the training process of the network by introducing an online rendering process. The rendering process not only renders images online as the input of network, but also efficiently back propagates gradients from the
                                        network to optimize the lighting pattern. Once trained, the network can estimate SVBRDFs from real photographs captured under the learned lighting pattern. Additionally, we describe an onsite capture setup that
                                        needs no careful calibration to capture the material sample efficiently. In particular, even a cell phone can be used for illumination. We demonstrate on synthetic and real data that our method could recover a wide
                                        range of materials from a single image casually captured under the learned lighting pattern.
                                    </p>
                                </div>
                                <div class="buttongroup ">
                                    <button class="btn btn-dark " data-toggle="collapse " data-target="#zhanglianghao2023 ">Abstract</button>
                                    <a href="https://klkjjhjkhjhg.github.io/sig23p1/ " class="btn btn-primary ">Project</a>
                                    <!-- <a href="# " class="btn btn-success ">Code (Coming soon) </a> -->
                                </div>
                                <!-- <a href="# " class="btn btn-secondary "></a> -->
                            </div>
                        </div>
                    </div>
                </div>
                <div class="papercontent container " style="padding: 0 ">
                    <div class="row ">
                        <div class="col-md-3 text-center ">
                            <img class="image papericon " src="imgs/gaofangzhou2023.png ">
                        </div>
                        <div class="col-md-9 ">
                            <p style="font-size: 18px; font-weight: bold; margin-bottom: 5px ">Transparent Object Reconstruction via Implicit Differentiable Refraction Rendering</p>
                            <hr style="margin-top:0px; margin-bottom:5px ">
                            <div class="text-left ">
                                <div>
                                    <p>
                                        Fangzhou Gao, <a href="# " style="color:#000; font-weight: bold; ">Lianghao Zhang</a>,
                                        <a href="https://cgliwang.github.io ">Li Wang</a>, Jiamin Cheng,
                                        <a href="http://cic.tju.edu.cn/faculty/zhangjiawan/index.htm ">Jiawan Zhang</a>
                                        <br> ACM SIGGRAPH Asia 2023 Conference Proceedings, Article No. 57, 1-11.
                                    </p>
                                </div>
                                <div id="project-details " class="collapse " , style="text-align: justify; ">
                                    <p>
                                        Reconstructing the geometry of transparent objects has been a long-standing challenge. Existing methods rely on complex setups, such as manual annotation or darkroom conditions, to obtain ob- ject silhouettes and usually require controlled environments
                                        with designed patterns to infer ray-background correspondence. How- ever, these intricate arrangements limit the practical application for common users. In this paper, we significantly simplify the setups and present
                                        a novel method that reconstructs transparent objects in unknown natural scenes without manual assistance. Our method incorporates two key technologies. Firstly, we introduce a volume rendering-based method that
                                        estimates object silhouettes by pro- jecting the 3D neural field onto 2D images. This automated process yields highly accurate multi-view object silhouettes from images captured in natural scenes. Secondly, we propose
                                        transparent ob- ject optimization through differentiable refraction rendering with the neural SDF field, enabling us to optimize the refraction ray based on color rather than explicit ray-background correspondence.
                                        Additionally, our optimization includes a ray sampling method to supervise the object silhouette at a low computational cost. Exten- sive experiments and comparisons demonstrate that our method produces high-quality
                                        results while offering much more convenient setups.
                                    </p>
                                </div>
                                <div class="buttongroup ">
                                    <button class="btn btn-dark " data-toggle="collapse " data-target="#project-details ">Abstract</button>
                                    <a href="https://arkgao.github.io/TransReconProjectWeb/ " class="btn btn-primary ">Project</a>
                                    <!-- <a href="# " class="btn btn-success ">Code (Coming soon) </a> -->
                                </div>
                                <!-- <a href="# " class="btn btn-secondary "></a> -->
                            </div>
                        </div>
                    </div>
                </div>
                <div class="papercontent container " style="padding: 0 ">
                    <div class="row ">
                        <div class="col-md-3 text-center ">
                            <img class="image papericon " src="imgs/wangli2023.png ">
                        </div>
                        <div class="col-md-9 ">
                            <p style="font-size: 18px; font-weight: bold; margin-bottom: 5px ">DeepBasis: Hand-Held Single-Image SVBRDF Capture via Two-Level Basis Material Model</p>
                            <hr style="margin-top:0px; margin-bottom:5px ">
                            <div class="text-left ">
                                <div>
                                    <p>
                                        <a href="https://cgliwang.github.io ">Li Wang</a>,
                                        <a href="# " style="color:#000; font-weight: bold; ">Lianghao Zhang</a>, Fangzhou Gao,
                                        <a href="http://cic.tju.edu.cn/faculty/zhangjiawan/index.htm ">Jiawan Zhang</a>
                                        <br> ACM SIGGRAPH Asia 2023 Conference Proceedings, Article No. 85, 1-11.
                                    </p>
                                </div>
                                <div id="wangli2023 " class="collapse " , style="text-align: justify; ">
                                    <p>
                                        Recovering spatial-varying bi-directional reflectance distribution function (SVBRDF) from a single hand-held captured image has been a meaningful but challenging task in computer graphics. Ben- efiting from the learned data priors, some previous methods
                                        can utilize the potential material correlations between image pixels to serve for SVBRDF estimation. To further reduce the ambigu- ity from single-image estimation, it is necessary to integrate ad- ditional explicit
                                        material correlations. Given the flexible expres- sive ability of basis material assumption, we propose DeepBasis, a deep-learning-based method integrated with this assumption. It jointly predicts basis materials
                                        and their blending weights. Then the estimated SVBRDF is their linear combination. To facilitate the extraction of data priors, we introduce a two-level basis model to keep the sufficient representative while using
                                        a fixed number of basis materials. Moreover, considering the absence of ground-truth basis materials and weights during network training, we propose a variance-consistency loss and adopt a joint prediction strategy,
                                        thereby enabling the existing SVBRDF dataset available for train- ing. Additionally, due to the hand-held capture setting, the exact lighting directions are unknown. We model the lighting direction es- timation
                                        as a sampling problem and propose an optimization-based algorithm to find the optimal estimation. Quantitative evaluation and qualitative analysis demonstrate that DeepBasis can produce a higher quality SVBRDF estimation
                                        than previous methods. All source codes will be publicly released.
                                    </p>
                                </div>
                                <button class="btn btn-dark " data-toggle="collapse " data-target="#wangli2023 ">Abstract</button>
                                <a href="https://cgliwang.github.io/DeepBasis/ " class="btn btn-primary ">Project</a>
                                <a href="https://github.com/CGLiWang/DeepBasis_SVBRDF " class="btn btn-light shadow-sm ">Code</a>
                                <!-- <a href="# " class="btn btn-secondary "></a> -->
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <div id="contact ">
                <div class="text-center " style="margin-top: 20px; ">
                    <p><a href="mailto:lianghaozhang@tju.edu.cn ">e-mail</a> | <a href="https://github.com/klkjjhjkhjhg ">github</a></p>
                </div>
            </div>
        </div>

        <script src="js/jquery-3.5.1.slim.min.js "></script>
        <script src="js/popper.min.js "></script>
        <script src="js/bootstrap.min.js "></script>

</body>

</html>